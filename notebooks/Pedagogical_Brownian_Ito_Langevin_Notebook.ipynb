{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brownian Motion, the CLT, Itô Calculus, and Langevin SDEs — a pedagogical notebook\n",
    "\n",
    "This notebook builds a coherent path from **classical calculus** and the **Central Limit Theorem (CLT)** to:\n",
    "\n",
    "- **Brownian motion** (Wiener process) as a scaling limit of random walks,\n",
    "- why Brownian paths break classical assumptions (non-differentiability, infinite variation),\n",
    "- **Itô’s lemma** and the **Itô correction** (the extra \\($\\tfrac12 \\sigma^2 f''$\\) term),\n",
    "- **overdamped Langevin dynamics** as *“gradient descent + noise”* and its stationary distribution.\n",
    "\n",
    "**Dependencies:** `numpy`, `matplotlib` (standard library `math` only).  \n",
    "All simulations are small-scale and designed to **verify** the mathematics visually and numerically.\n",
    "\n",
    "---\n",
    "\n",
    "## Notation (used consistently)\n",
    "\n",
    "- $t$: time, $dt$: an infinitesimal time increment.\n",
    "- $W_t$: standard Brownian motion (Wiener process).\n",
    "- $dW_t$: Brownian increment over $[t,t+dt]$, heuristically $dW_t \\sim \\mathcal N(0, dt)$.\n",
    "- SDE (1D):  \n",
    "  $$\n",
    "  dX_t = \\mu(X_t,t)\\,dt + \\sigma(X_t,t)\\,dW_t\n",
    "  $$\n",
    "  where $\\mu$ is **drift** and $\\sigma$ is **diffusion scale**.\n",
    "- Langevin (1D/ \\(d\\)D):  \n",
    "  $$\n",
    "  dX_t = -\\nabla U(X_t)\\,dt + \\sqrt{2D}\\,dW_t\n",
    "  $$\n",
    "  where $U$ is an **energy/potential** and $D>0$ is the **diffusion coefficient**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning objectives (what you should be able to explain by the end)\n",
    "\n",
    "1. How the **CLT** motivates **Gaussian increments**.\n",
    "2. Why Brownian motion has **$\\sqrt{dt}$** scaling.\n",
    "3. Exactly why classical derivatives/chain rule fail for Brownian paths.\n",
    "4. Where the $\\tfrac12\\sigma^2 f''$ term in **Itô’s lemma** comes from.\n",
    "5. The geometric meaning of Langevin SDE as **energy descent + stochastic exploration** and why\n",
    "   $$\n",
    "   p_\\infty(x)\\propto e^{-U(x)/D}.\n",
    "   $$\n"
   ],
   "id": "2b1abac6b4aa6846"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T06:35:24.712151Z",
     "start_time": "2026-01-08T06:35:24.282665Z"
    }
   },
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# For reproducibility (feel free to change)\nrng = np.random.default_rng(0)\n\nplt.rcParams[\"figure.figsize\"] = (8, 4.5)\n",
   "id": "53c36773b6e4a221",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Classical calculus recap: what works for smooth paths\n",
    "\n",
    "### Limits, derivatives, Taylor expansion\n",
    "\n",
    "If $x(t)$ is differentiable at time $t$, then for small $dt$,\n",
    "\n",
    "$$\n",
    "x(t+dt)=x(t)+x'(t)\\,dt + o(dt).\n",
    "$$\n",
    "\n",
    "For a *twice differentiable* scalar function $f:\\mathbb R\\to\\mathbb R$, Taylor’s theorem gives\n",
    "\n",
    "$$\n",
    "f(x+\\Delta x)=f(x)+f'(x)\\Delta x+\\tfrac12 f''(x)(\\Delta x)^2+o((\\Delta x)^2).\n",
    "$$\n",
    "\n",
    "### Classical chain rule\n",
    "\n",
    "If $x(t)$ is differentiable and $f$ is differentiable, then\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt}f(x(t)) = f'(x(t))\\,x'(t).\n",
    "$$\n",
    "\n",
    "**Hidden assumption:** $x(t)$ has increments $\\Delta x = x(t+dt)-x(t)$ that are order $dt$, i.e. $\\Delta x = O(dt)$.\n",
    "Then $(\\Delta x)^2=O(dt^2)$ is negligible at first order, which is why classical calculus ignores it.\n",
    "\n",
    "### What will break for Brownian motion?\n",
    "\n",
    "Brownian increments scale like $\\Delta W = O(\\sqrt{dt})$, so $(\\Delta W)^2=O(dt)$ — **not negligible**.\n",
    "That single change forces a new chain rule (Itô’s lemma).\n"
   ],
   "id": "296bedbbc2be8836"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# A tiny numerical reminder: for smooth x(t)=t^2, increments are O(dt)\nT = 1.0\nfor dt in [1e-1, 1e-2, 1e-3, 1e-4]:\n    t = 0.3\n    dx = (t+dt)**2 - t**2\n    print(f\"dt={dt:>7.0e}  dx≈{dx:>10.3e}  dx/dt≈{dx/dt:>10.3e}  (dx)^2/dt≈{dx*dx/dt:>10.3e}\")\n",
   "id": "8d2451eaad26685e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def normal_pdf(x, mu=0.0, sigma=1.0):\n    return (1.0/(sigma*math.sqrt(2*math.pi))) * np.exp(-0.5*((x-mu)/sigma)**2)\n\ndef normal_cdf(x):\n    # Standard normal CDF via erf (no scipy)\n    return 0.5*(1.0 + np.vectorize(math.erf)(x/math.sqrt(2.0)))\n\ndef ks_statistic(samples):\n    # One-sample KS statistic vs N(0,1)\n    x = np.sort(samples)\n    n = len(x)\n    ecdf = np.arange(1, n+1)/n\n    cdf = normal_cdf(x)\n    return np.max(np.abs(ecdf - cdf))\n\ndef clt_demo(dist_name, sampler, ns=(1,2,5,10,30,100), N=20000):\n    fig, axes = plt.subplots(2, 3, figsize=(12, 7))\n    axes = axes.ravel()\n    for i, n in enumerate(ns):\n        X = sampler(size=(N, n))\n        m = X.mean(axis=1)\n        # normalize: sqrt(n)*(mean - E[X]) / std(X)\n        EX = sampler(size=(200000,)).mean()          # quick Monte Carlo estimate for EX\n        SD = sampler(size=(200000,)).std(ddof=0)     # estimate for SD\n        Z = math.sqrt(n)*(m - EX)/SD\n        ax = axes[i]\n        ax.hist(Z, bins=60, density=True, alpha=0.7)\n        xs = np.linspace(-4, 4, 400)\n        ax.plot(xs, normal_pdf(xs))\n        ax.set_title(f\"{dist_name}, n={n}  KS≈{ks_statistic(Z):.3f}\")\n        ax.set_xlim(-4, 4)\n    fig.suptitle(\"CLT: normalized sample means converge to N(0,1)\")\n    plt.tight_layout()\n    plt.show()\n\n# Distributions\ndef sample_uniform(size):     # Uniform(0,1)\n    return rng.random(size=size)\n\ndef sample_bernoulli(size):   # Bernoulli(p=0.3)\n    return rng.binomial(1, 0.3, size=size)\n\ndef sample_exponential(size): # Exp(rate=1)\n    return rng.exponential(scale=1.0, size=size)\n\nclt_demo(\"Uniform(0,1)\", sample_uniform)\nclt_demo(\"Bernoulli(0.3)\", sample_bernoulli)\nclt_demo(\"Exponential(1)\", sample_exponential)\n",
   "id": "7a0c55e130d9be06"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Random walk $\\to$ Brownian motion: scaling and invariance intuition\n",
    "\n",
    "### Symmetric random walk\n",
    "\n",
    "Let $\\xi_k\\in\\{+1,-1\\}$ with $\\mathbb P(\\xi_k=1)=\\mathbb P(\\xi_k=-1)=\\tfrac12$, i.i.d.\n",
    "Define partial sums:\n",
    "\n",
    "$$\n",
    "S_n=\\sum_{k=1}^n \\xi_k.\n",
    "$$\n",
    "\n",
    "Then $\\mathbb E[S_n]=0$ and $\\mathrm{Var}(S_n)=n$.\n",
    "\n",
    "### How to scale time and space\n",
    "\n",
    "Suppose each step takes time $\\Delta t$. After $n$ steps, time is $t=n\\Delta t$.\n",
    "\n",
    "If you also scale the spatial step by $\\Delta x$, then position after $n$ steps is\n",
    "\n",
    "$$\n",
    "X_t = \\Delta x \\, S_n.\n",
    "$$\n",
    "\n",
    "Its variance is\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X_t) = (\\Delta x)^2 \\mathrm{Var}(S_n)\n",
    "= (\\Delta x)^2 n\n",
    "= (\\Delta x)^2 \\frac{t}{\\Delta t}.\n",
    "$$\n",
    "\n",
    "To obtain a **nontrivial** finite limit as $\\Delta t\\to 0$ (many steps in finite time), you want $\\mathrm{Var}(X_t)\\propto t$.\n",
    "This forces\n",
    "\n",
    "$$\n",
    "(\\Delta x)^2 \\frac{1}{\\Delta t} = \\text{constant}\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\Delta x \\propto \\sqrt{\\Delta t}.\n",
    "$$\n",
    "\n",
    "This is the origin of the famous **$\\sqrt{dt}$** scaling.\n",
    "\n",
    "### Diffusion coefficient $D$\n",
    "\n",
    "A standard Brownian motion $W_t$ satisfies\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(W_{t+\\Delta t}-W_t)=\\Delta t.\n",
    "$$\n",
    "\n",
    "A general diffusion with coefficient $D$ has\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X_{t+\\Delta t}-X_t)=2D\\,\\Delta t.\n",
    "$$\n",
    "\n",
    "So you can model it as $X_t=\\sqrt{2D}\\,W_t$, i.e. scale Brownian motion by $\\sqrt{2D}$.\n"
   ],
   "id": "b93b43635a57cdd1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def simulate_random_walk_paths(n_steps=2000, n_paths=20, dt=1e-3):\n    # +/-1 steps\n    xi = rng.choice([-1, 1], size=(n_paths, n_steps))\n    S = np.cumsum(xi, axis=1)\n    t = np.arange(1, n_steps+1)*dt\n    # Brownian scaling: dx = sqrt(dt)\n    X = np.sqrt(dt) * S\n    return t, X\n\nt, X = simulate_random_walk_paths(n_steps=3000, n_paths=30, dt=1e-3)\n\nplt.figure()\nfor i in range(X.shape[0]):\n    plt.plot(t, X[i], alpha=0.7)\nplt.title(\"Rescaled random walks approximate Brownian motion (many sample paths)\")\nplt.xlabel(\"t\")\nplt.ylabel(\"X_t\")\nplt.show()\n",
   "id": "2c8af31604ece7cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Brownian motion properties: increments, variance, non-differentiability, quadratic variation\n",
    "\n",
    "### Definition (standard Brownian motion)\n",
    "\n",
    "A process $(W_t)_{t\\ge0}$ is a standard Brownian motion if:\n",
    "\n",
    "1. $W_0=0$ a.s.\n",
    "2. **Independent increments:** $W_{t_2}-W_{t_1}$ is independent of the past for $t_2>t_1$.\n",
    "3. **Gaussian increments:** $W_{t+\\Delta t}-W_t\\sim\\mathcal N(0,\\Delta t)$.\n",
    "4. **Continuous paths:** $t\\mapsto W_t$ is continuous a.s.\n",
    "\n",
    "### Non-differentiability in one diagnostic\n",
    "\n",
    "A key signature is the scaling:\n",
    "\n",
    "$$\n",
    "\\frac{W_{t+dt}-W_t}{dt} \\sim \\mathcal N\\left(0,\\frac{1}{dt}\\right),\n",
    "$$\n",
    "\n",
    "whose variance blows up as $dt\\to 0$. So the “velocity” becomes unbounded — classical derivatives fail.\n",
    "\n",
    "### Quadratic variation\n",
    "\n",
    "For a partition $0=t_0<t_1<\\dots<t_N=T$, define\n",
    "\n",
    "$$\n",
    "[W]_T^{(N)} := \\sum_{k=0}^{N-1} (W_{t_{k+1}}-W_{t_k})^2.\n",
    "$$\n",
    "\n",
    "A fundamental theorem is that as the mesh $\\max_k(t_{k+1}-t_k)\\to 0$,\n",
    "\n",
    "$$\n",
    "[W]_T^{(N)} \\to T \\quad\\text{(in probability, and almost surely along refinements)}.\n",
    "$$\n",
    "\n",
    "This *finite, nonzero quadratic variation* is exactly why $(dW_t)^2$ behaves like $dt$ in Itô calculus.\n"
   ],
   "id": "ed05c307e4532496"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def simulate_brownian(T=1.0, dt=1e-3, n_paths=500):\n    n = int(T/dt)\n    dW = math.sqrt(dt) * rng.standard_normal(size=(n_paths, n))\n    W = np.cumsum(dW, axis=1)\n    t = np.arange(1, n+1)*dt\n    return t, W, dW\n\nT = 1.0\ndt = 1e-3\nt, W, dW = simulate_brownian(T=T, dt=dt, n_paths=200)\n\n# (a) Increments look Gaussian with variance dt\ninc = dW[:, 0]  # increments over the first dt\nprint(\"Empirical mean(dW) ≈\", inc.mean(), \"  empirical var(dW) ≈\", inc.var(), \"  target var =\", dt)\n\nplt.figure()\nplt.hist(inc/ math.sqrt(dt), bins=60, density=True, alpha=0.7)\nxs = np.linspace(-4,4,400)\nplt.plot(xs, normal_pdf(xs))\nplt.title(\"Standardized increments dW/sqrt(dt) ≈ N(0,1)\")\nplt.xlabel(\"z\")\nplt.ylabel(\"density\")\nplt.show()\n\n# (b) Quadratic variation check\nqv = np.sum(dW**2, axis=1)  # sum of squared increments over [0,T]\nprint(\"Quadratic variation: mean ≈\", qv.mean(), \"  std ≈\", qv.std(), \"  target =\", T)\n\nplt.figure()\nplt.hist(qv, bins=60, density=True, alpha=0.7)\nplt.title(\"Quadratic variation estimates across paths (should concentrate near T)\")\nplt.xlabel(\"sum (dW)^2\")\nplt.ylabel(\"density\")\nplt.show()\n",
   "id": "feab73d391385870"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Zoom-in self-similarity and non-differentiability (visual)\n\nA differentiable curve “looks straight” when you zoom in enough. A Brownian path never settles into a straight line; it remains jagged at every scale.\nWe'll plot a single Brownian path and zoom into a small interval.\n",
   "id": "39450f987dc5e698"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "t, W1, _ = simulate_brownian(T=1.0, dt=1e-4, n_paths=1)\nW1 = W1[0]\n\nplt.figure()\nplt.plot(t, W1)\nplt.title(\"One Brownian path over [0,1]\")\nplt.xlabel(\"t\")\nplt.ylabel(\"W_t\")\nplt.show()\n\n# Zoom into a short interval\na, b = 0.40, 0.42\nmask = (t >= a) & (t <= b)\n\nplt.figure()\nplt.plot(t[mask], W1[mask])\nplt.title(f\"Zoom-in: Brownian path on [{a},{b}] (still jagged)\")\nplt.xlabel(\"t\")\nplt.ylabel(\"W_t\")\nplt.show()\n",
   "id": "1f6d8232c3da878a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) Why the classical chain rule changes: \\(dW_t^2 \\sim dt\\)\n\nConsider an Itô process\n\n\\[\ndX_t = \\mu(X_t,t)\\,dt + \\sigma(X_t,t)\\,dW_t.\n\\]\n\nOver a small interval \\(dt\\), the increment is\n\n\\[\n\\Delta X \\approx \\mu\\,dt + \\sigma\\,\\Delta W,\n\\quad\\text{where}\\quad\n\\Delta W \\sim \\mathcal N(0,dt).\n\\]\n\nKey scaling:\n\n- \\(\\Delta W = O(\\sqrt{dt})\\)\n- so \\((\\Delta W)^2 = O(dt)\\)\n- while \\(dt\\cdot \\Delta W = O(dt^{3/2})\\) (negligible vs \\(dt\\))\n- and \\(dt^2 = O(dt^2)\\) (also negligible)\n\n### Derivation sketch from Taylor expansion\n\nLet \\(f\\) be \\(C^2\\). Taylor expand to second order:\n\n\\[\nf(X_{t+dt})-f(X_t)\n\\approx f'(X_t)\\Delta X + \\tfrac12 f''(X_t) (\\Delta X)^2.\n\\]\n\nNow expand \\((\\Delta X)^2\\):\n\n\\[\n(\\Delta X)^2 \\approx (\\mu dt + \\sigma \\Delta W)^2\n= \\mu^2 dt^2 + 2\\mu\\sigma\\,dt\\,\\Delta W + \\sigma^2 (\\Delta W)^2.\n\\]\n\nAs \\(dt\\to 0\\), the first two terms vanish faster than \\(dt\\), but the last term is order \\(dt\\).\nThus **the second-order Taylor term contributes at first order in time**:\n\n\\[\n\\tfrac12 f''(X_t)(\\Delta X)^2 \\approx \\tfrac12 f''(X_t)\\sigma^2 (\\Delta W)^2 \\approx \\tfrac12 f''(X_t)\\sigma^2 dt.\n\\]\n\nThat is the “Itô correction” and the precise reason Itô calculus differs from classical calculus.\n\nWe'll verify the scaling numerically next.\n",
   "id": "c53fd10815ed67cb"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Verify scaling: for Brownian increments, (dW)^2 behaves like dt in expectation and concentrates as dt->0.\n\ndef scaling_experiment(dts=(1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 5e-4), N=200000):\n    rows = []\n    for dt in dts:\n        dW = math.sqrt(dt) * rng.standard_normal(size=N)\n        rows.append((dt,\n                     dW.mean(),\n                     dW.var(),\n                     (dW**2).mean(),\n                     (dW**2).var(),\n                     np.mean(np.abs(dW))/math.sqrt(dt)))\n    return rows\n\nrows = scaling_experiment()\nprint(\"dt      E[dW]      Var[dW]     E[(dW)^2]  Var[(dW)^2]  E|dW|/sqrt(dt)\")\nfor r in rows:\n    print(f\"{r[0]:.0e}  {r[1]: .3e}  {r[2]: .3e}  {r[3]: .3e}  {r[4]: .3e}  {r[5]: .3f}\")\n",
   "id": "6d171889b3b087d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6) Itô’s lemma: derivation, examples, and numerical verification\n\n### Derivation (1D)\n\nLet \\(X_t\\) satisfy\n\n\\[\ndX_t = \\mu(X_t,t)\\,dt + \\sigma(X_t,t)\\,dW_t.\n\\]\n\nLet \\(f(x,t)\\) be \\(C^{2,1}\\): twice differentiable in \\(x\\), once in \\(t\\).  \nDo a second-order Taylor expansion in both variables:\n\n\\[\ndf = f(X_{t+dt},t+dt)-f(X_t,t)\n\\approx f_t\\,dt + f_x\\,\\Delta X + \\tfrac12 f_{xx}(\\Delta X)^2,\n\\]\n\nwhere derivatives are evaluated at \\((X_t,t)\\) and \\(\\Delta X = \\mu dt + \\sigma \\Delta W\\).\n\nUsing the scaling from the previous section:\n\n- \\((\\Delta W)^2 \\approx dt\\),\n- \\(dt\\Delta W\\) and \\(dt^2\\) are negligible.\n\nSo\n\n\\[\n(\\Delta X)^2 \\approx \\sigma^2 (\\Delta W)^2 \\approx \\sigma^2 dt.\n\\]\n\nSubstitute:\n\n\\[\ndf \\approx \\left(f_t + \\mu f_x + \\tfrac12 \\sigma^2 f_{xx}\\right)dt + \\sigma f_x\\,dW_t.\n\\]\n\nThis is **Itô’s lemma**:\n\n\\[\n\\boxed{\ndf(X_t,t) =\n\\left(\\partial_t f + \\mu\\,\\partial_x f + \\tfrac12\\sigma^2\\,\\partial_{xx}f\\right)dt\n+ \\sigma\\,\\partial_x f\\, dW_t.\n}\n\\]\n\n---\n\n### Worked example 1: \\(f(x)=x^2\\) for Brownian motion\n\nTake \\(X_t=W_t\\), so \\(\\mu=0,\\sigma=1\\).  \nThen \\(f_x=2x,\\ f_{xx}=2\\). Itô gives:\n\n\\[\nd(W_t^2) = 2W_t\\,dW_t + 1\\cdot dt.\n\\]\n\nIntegrating from \\(0\\) to \\(T\\):\n\n\\[\nW_T^2 - W_0^2 = \\int_0^T 2W_t\\,dW_t + \\int_0^T dt.\n\\]\n\nSo\n\\[\nW_T^2 - T\n\\]\nis a martingale (mean 0), and \\(\\mathbb E[W_T^2]=T\\).\n\n### Worked example 2: \\(f(x)=e^{ax}\\) for Brownian motion\n\nAgain \\(X_t=W_t\\), \\(\\mu=0,\\sigma=1\\).\nHere \\(f_x = a e^{ax},\\ f_{xx}=a^2 e^{ax}\\). Then\n\n\\[\nd(e^{aW_t}) = a e^{aW_t}\\,dW_t + \\tfrac12 a^2 e^{aW_t}\\,dt.\n\\]\n\nA useful corollary is that\n\\[\nM_t = \\exp\\left(aW_t - \\tfrac12 a^2 t\\right)\n\\]\nis a martingale.\n\n### Worked example 3 (time-dependent): \\(f(x,t)=\\sin(x)\\,e^{-t/2}\\) with \\(X_t=W_t\\)\n\nCompute derivatives:\n- \\(f_t = -\\tfrac12 \\sin(x)e^{-t/2}\\),\n- \\(f_x = \\cos(x)e^{-t/2}\\),\n- \\(f_{xx} = -\\sin(x)e^{-t/2}\\).\n\nPlug into Itô with \\(\\mu=0,\\sigma=1\\):\n\n\\[\ndf = \\left(f_t + \\tfrac12 f_{xx}\\right)dt + f_x dW_t\n= \\left(-\\tfrac12\\sin(x)e^{-t/2} + \\tfrac12(-\\sin(x)e^{-t/2})\\right)dt + \\cos(x)e^{-t/2}\\,dW_t\n= -\\sin(x)e^{-t/2}\\,dt + \\cos(x)e^{-t/2}\\,dW_t.\n\\]\n\nSo the drift term is \\(-\\sin(W_t)e^{-t/2}\\).\n",
   "id": "8ba69ef77c538f2b"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def euler_maruyama(mu, sigma, x0, T, dt):\n    n = int(T/dt)\n    t = np.arange(0, n+1)*dt\n    X = np.empty(n+1)\n    X[0] = x0\n    dW = math.sqrt(dt)*rng.standard_normal(size=n)\n    for k in range(n):\n        tk = t[k]\n        xk = X[k]\n        X[k+1] = xk + mu(xk, tk)*dt + sigma(xk, tk)*dW[k]\n    return t, X, dW\n\ndef ito_verify(f, fx, fxx, ft, mu, sigma, x0=0.0, T=1.0, dt=1e-3):\n    t, X, dW = euler_maruyama(mu, sigma, x0, T, dt)\n    # LHS\n    lhs = f(X[-1], T) - f(X[0], 0.0)\n    # RHS (discretized)\n    drift = 0.0\n    mart = 0.0\n    for k in range(len(dW)):\n        tk = t[k]\n        xk = X[k]\n        drift += (ft(xk, tk) + mu(xk, tk)*fx(xk, tk) + 0.5*(sigma(xk, tk)**2)*fxx(xk, tk))*dt\n        mart  += sigma(xk, tk)*fx(xk, tk)*dW[k]\n    rhs = drift + mart\n    return lhs, rhs, lhs-rhs\n\n# Example 1: f(x)=x^2, X=W (mu=0, sigma=1)\nmu0 = lambda x,t: 0.0\nsig1 = lambda x,t: 1.0\n\nf1   = lambda x,t: x**2\nfx1  = lambda x,t: 2*x\nfxx1 = lambda x,t: 2.0\nft1  = lambda x,t: 0.0\n\n# Run multiple paths and check error vs dt\ndef error_vs_dt(dts, n_paths=200):\n    errs = []\n    for dt in dts:\n        e = []\n        for _ in range(n_paths):\n            lhs, rhs, err = ito_verify(f1, fx1, fxx1, ft1, mu0, sig1, x0=0.0, T=1.0, dt=dt)\n            e.append(err)\n        errs.append((dt, float(np.mean(np.abs(e))), float(np.std(e))))\n    return errs\n\ndts = [1e-1, 5e-2, 2e-2, 1e-2, 5e-3, 2e-3, 1e-3]\nerrs = error_vs_dt(dts, n_paths=300)\n\nprint(\"dt      mean|error|   std(error)\")\nfor dt, m, s in errs:\n    print(f\"{dt:>7.0e}   {m:>10.3e}   {s:>10.3e}\")\n\nplt.figure()\nplt.plot([e[0] for e in errs], [e[1] for e in errs], marker=\"o\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.title(\"Itô verification for f(x)=x^2, X=W: error decreases as dt decreases\")\nplt.xlabel(\"dt\")\nplt.ylabel(\"mean absolute error\")\nplt.show()\n",
   "id": "3db19e8bf80ec8c8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Example 2: f(x)=exp(a x), X=W\na = 0.8\nf2   = lambda x,t: math.exp(a*x)\nfx2  = lambda x,t: a*math.exp(a*x)\nfxx2 = lambda x,t: (a*a)*math.exp(a*x)\nft2  = lambda x,t: 0.0\n\n# Single-path demonstration: compare both sides\nlhs, rhs, err = ito_verify(f2, fx2, fxx2, ft2, mu0, sig1, x0=0.0, T=1.0, dt=1e-4)\nprint(\"Example 2 (single path): LHS, RHS, error =\", lhs, rhs, err)\n\n# Many paths: error statistics\nerrs = []\nfor _ in range(500):\n    _, _, e = ito_verify(f2, fx2, fxx2, ft2, mu0, sig1, x0=0.0, T=1.0, dt=1e-3)\n    errs.append(e)\nerrs = np.array(errs)\nprint(\"Example 2: mean error =\", errs.mean(), \"  mean |error| =\", np.mean(np.abs(errs)))\n",
   "id": "739e520d0b45f087"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Example 3: time-dependent f(x,t)=sin(x)*exp(-t/2), X=W\nf3   = lambda x,t: math.sin(x)*math.exp(-t/2.0)\nfx3  = lambda x,t: math.cos(x)*math.exp(-t/2.0)\nfxx3 = lambda x,t: -math.sin(x)*math.exp(-t/2.0)\nft3  = lambda x,t: -0.5*math.sin(x)*math.exp(-t/2.0)\n\nlhs, rhs, err = ito_verify(f3, fx3, fxx3, ft3, mu0, sig1, x0=0.0, T=1.0, dt=1e-4)\nprint(\"Example 3 (single path): error =\", err)\n\n# Error vs dt\ndts = [5e-2, 2e-2, 1e-2, 5e-3, 2e-3, 1e-3]\nerrs = []\nfor dt in dts:\n    e = []\n    for _ in range(300):\n        _, _, ee = ito_verify(f3, fx3, fxx3, ft3, mu0, sig1, x0=0.0, T=1.0, dt=dt)\n        e.append(ee)\n    errs.append((dt, float(np.mean(np.abs(e)))))\nplt.figure()\nplt.plot([d for d,_ in errs], [m for _,m in errs], marker=\"o\")\nplt.xscale(\"log\"); plt.yscale(\"log\")\nplt.title(\"Itô verification for time-dependent example: error vs dt\")\nplt.xlabel(\"dt\"); plt.ylabel(\"mean |error|\")\nplt.show()\n",
   "id": "d3f68bf4314070e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7) From Itô to Langevin: “gradient flow + noise” and equilibrium density\n\n### Overdamped Langevin dynamics\n\nIn \\(d\\) dimensions:\n\n\\[\ndX_t = -\\nabla U(X_t)\\,dt + \\sqrt{2D}\\,dW_t,\n\\]\n\nwhere \\(W_t\\) is \\(d\\)-dimensional Brownian motion (independent components).\n\n**Interpretation:**\n- Drift \\(-\\nabla U\\): a vector field pointing “downhill” in the energy landscape \\(U\\).  \n  This is precisely the same direction used by gradient descent.\n- Diffusion \\(\\sqrt{2D}\\,dW_t\\): isotropic random perturbations that explore space.\n- The competition yields sampling concentrated in low-\\(U\\) regions but still able to cross barriers.\n\n### Fokker–Planck derivation of the stationary distribution\n\nLet \\(p(x,t)\\) be the probability density of \\(X_t\\).\nFor an Itô SDE\n\n\\[\ndX_t = b(X_t)\\,dt + \\sqrt{2D}\\,dW_t\n\\]\n(with constant isotropic diffusion), the Fokker–Planck equation is\n\n\\[\n\\partial_t p = -\\nabla\\cdot(b\\,p) + D\\,\\Delta p.\n\\]\n\nFor Langevin, \\(b(x)=-\\nabla U(x)\\), so\n\n\\[\n\\partial_t p = \\nabla\\cdot\\left(p\\,\\nabla U\\right) + D\\,\\Delta p.\n\\]\n\nA stationary density \\(p_\\infty\\) satisfies \\(\\partial_t p_\\infty=0\\):\n\n\\[\n0 = \\nabla\\cdot\\left(p_\\infty \\nabla U\\right) + D\\,\\Delta p_\\infty\n= \\nabla\\cdot\\left(p_\\infty \\nabla U + D\\,\\nabla p_\\infty\\right).\n\\]\n\nA sufficient (and standard) way to solve this is to set the probability **current** to zero:\n\n\\[\nJ(x) := -b(x)p_\\infty(x) - D\\nabla p_\\infty(x) = 0.\n\\]\n\nSince \\(b=-\\nabla U\\), this condition becomes\n\n\\[\n\\nabla U\\,p_\\infty + D\\nabla p_\\infty = 0\n\\quad\\Longrightarrow\\quad\n\\nabla p_\\infty = -\\frac{1}{D}p_\\infty \\nabla U.\n\\]\n\nDivide by \\(p_\\infty>0\\):\n\n\\[\n\\nabla \\log p_\\infty = -\\frac{1}{D}\\nabla U\n\\quad\\Longrightarrow\\quad\n\\log p_\\infty = -\\frac{1}{D}U + C\n\\]\n\nso\n\n\\[\n\\boxed{p_\\infty(x) \\propto e^{-U(x)/D}.}\n\\]\n\nThis is a Boltzmann/Gibbs density with “temperature” proportional to \\(D\\).\n",
   "id": "851ad2563650d9d4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8) Geometric visuals: double-well Langevin in 1D\n\nWe use a classic **double-well** potential:\n\n\\[\nU(x) = \\frac{1}{4}(x^2-1)^2.\n\\]\n\n- Minima near \\(x=\\pm 1\\) (two wells),\n- A barrier at \\(x=0\\).\n\nThe Langevin SDE is\n\n\\[\ndX_t = -U'(X_t)\\,dt + \\sqrt{2D}\\,dW_t.\n\\]\n\nWe will:\n1. Simulate trajectories and show **barrier crossing**.\n2. Compare the empirical histogram to the theoretical stationary density \\(p_\\infty(x)\\propto e^{-U(x)/D}\\).\n3. Visualize the drift field \\(-U'(x)\\) as a geometric vector field (in 1D: arrows along the line).\n\n### Sensible defaults\n- \\(D=0.2\\) (enough noise to cross the barrier sometimes),\n- small time step (Euler–Maruyama),\n- long run with a burn-in to approximate stationarity.\n",
   "id": "419fd3eb4631d17"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def U(x):\n    return 0.25*(x*x - 1.0)**2\n\ndef dU(x):\n    # derivative of 0.25*(x^2-1)^2 is x*(x^2-1)\n    return x*(x*x - 1.0)\n\ndef langevin_1d(x0=0.0, T=50.0, dt=1e-3, D=0.2):\n    n = int(T/dt)\n    t = np.arange(0, n+1)*dt\n    X = np.empty(n+1)\n    X[0] = x0\n    dW = math.sqrt(dt) * rng.standard_normal(size=n)\n    noise_scale = math.sqrt(2.0*D)\n    for k in range(n):\n        xk = X[k]\n        X[k+1] = xk - dU(xk)*dt + noise_scale*dW[k]\n    return t, X\n\n# Simulate a few trajectories\nD = 0.2\ndt = 2e-3\nT = 25.0\n\nplt.figure()\nfor x0 in [-1.0, -0.2, 0.2, 1.0]:\n    t, X = langevin_1d(x0=x0, T=T, dt=dt, D=D)\n    plt.plot(t, X, alpha=0.8, label=f\"x0={x0}\")\nplt.title(\"Langevin trajectories in a double well (barrier crossing may occur)\")\nplt.xlabel(\"t\")\nplt.ylabel(\"X_t\")\nplt.legend()\nplt.show()\n",
   "id": "8ebb1608a8307aa0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Drift field visualization: -U'(x)\nxs = np.linspace(-2.0, 2.0, 200)\ndrift = -dU(xs)\n\nplt.figure()\nplt.plot(xs, drift)\nplt.axhline(0.0)\nplt.title(\"Drift field b(x) = -U'(x): arrows point downhill in U\")\nplt.xlabel(\"x\")\nplt.ylabel(\"b(x)\")\nplt.show()\n\nplt.figure()\nplt.plot(xs, U(xs))\nplt.title(\"Double-well potential U(x)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"U(x)\")\nplt.show()\n",
   "id": "c2af0906c43fa3fd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Long run to estimate stationary density\nD = 0.2\ndt = 1e-3\nT = 200.0\n\nt, X = langevin_1d(x0=0.0, T=T, dt=dt, D=D)\n\nburn_in = int(0.2*len(X))\nsamples = X[burn_in:]  # approximate stationarity samples\n\n# Empirical histogram\nplt.figure()\ncounts, bins, _ = plt.hist(samples, bins=120, density=True, alpha=0.7, label=\"empirical\")\n\n# Theoretical stationary density proportional to exp(-U/D)\ngrid = np.linspace(-2.2, 2.2, 800)\np_unnorm = np.exp(-U(grid)/D)\n# Normalize numerically by trapezoid rule\nZ = np.trapz(p_unnorm, grid)\np = p_unnorm / Z\nplt.plot(grid, p, label=r\"$\\propto e^{-U(x)/D}$ (normalized)\")\n\nplt.title(f\"Stationary density check (D={D})\")\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.legend()\nplt.show()\n\nprint(\"Empirical mean:\", samples.mean(), \"Empirical var:\", samples.var())\n",
   "id": "9380ed11ab4405e1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Optional: “density over time” as a geometric evolution (coarse)\n\nThe Fokker–Planck equation describes how an initial distribution flows under:\n- deterministic transport by drift (downhill flow),\n- plus diffusion (spreading).\n\nWe can visualize this coarsely by histogram snapshots over time.\n",
   "id": "72467761ae95fc7b"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Coarse density evolution snapshots\nD = 0.2\ndt = 1e-3\nT = 20.0\nt, X = langevin_1d(x0=1.5, T=T, dt=dt, D=D)  # start away from wells\n\nsnap_times = [0.2, 1.0, 3.0, 8.0, 15.0, 20.0]\nsnap_idx = [int(st/dt) for st in snap_times]\nbins = np.linspace(-2.2, 2.2, 120)\n\nplt.figure(figsize=(10,6))\nfor st, idx in zip(snap_times, snap_idx):\n    window = X[max(0, idx-2000):idx+1]  # local window as a proxy for \"distribution\"\n    hist, edges = np.histogram(window, bins=bins, density=True)\n    centers = 0.5*(edges[:-1]+edges[1:])\n    plt.plot(centers, hist, alpha=0.8, label=f\"t≈{st}\")\nplt.plot(grid, p, label=\"stationary (theory)\")\nplt.title(\"Coarse density snapshots (using sliding-window histograms)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.legend()\nplt.show()\n",
   "id": "401033f0c597c4c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) The Central Limit Theorem (CLT): derivation outline + simulation\n",
    "\n",
    "### Statement (classical i.i.d. CLT)\n",
    "\n",
    "Let $(X_k)_{k\\ge1}$ be i.i.d. with mean \\(\\mathbb E[X_k]=m\\) and variance \\(\\mathrm{Var}(X_k)=s^2\\in(0,\\infty)\\).\n",
    "Define the normalized sum\n",
    "\n",
    "$$\n",
    "Z_n=\\frac{\\sum_{k=1}^n X_k - n m}{s\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "Then as \\(n\\to\\infty\\), \\(Z_n \\Rightarrow \\mathcal N(0,1)\\) (convergence in distribution).\n",
    "\n",
    "### A clean derivation outline using moment generating functions (MGFs)\n",
    "\n",
    "Let \\(Y_k = \\frac{X_k-m}{s}\\) so that \\(\\mathbb E[Y_k]=0\\), \\(\\mathrm{Var}(Y_k)=1\\).\n",
    "Then\n",
    "\n",
    "\\[\n",
    "Z_n = \\frac{1}{\\sqrt n}\\sum_{k=1}^n Y_k.\n",
    "\\]\n",
    "\n",
    "Let \\(M_Y(t)=\\mathbb E[e^{tY}]\\) be the MGF of \\(Y\\), assumed finite near \\(t=0\\).\n",
    "Because \\(\\mathbb E[Y]=0\\) and \\(\\mathbb E[Y^2]=1\\), we have a Taylor expansion around \\(0\\):\n",
    "\n",
    "\\[\n",
    "M_Y(t)=1+\\frac{t^2}{2}+o(t^2).\n",
    "\\]\n",
    "\n",
    "Now compute the MGF of \\(Z_n\\):\n",
    "\n",
    "\\[\n",
    "M_{Z_n}(t)=\\mathbb E\\left[e^{t Z_n}\\right]\n",
    "=\\mathbb E\\left[\\exp\\left(\\frac{t}{\\sqrt n}\\sum_{k=1}^n Y_k\\right)\\right]\n",
    "=\\prod_{k=1}^n \\mathbb E\\left[e^{\\frac{t}{\\sqrt n} Y_k}\\right]\n",
    "=\\left(M_Y\\left(\\frac{t}{\\sqrt n}\\right)\\right)^n.\n",
    "\\]\n",
    "\n",
    "Using the expansion \\(M_Y(u)=1+\\tfrac{u^2}{2}+o(u^2)\\) with \\(u=t/\\sqrt n\\):\n",
    "\n",
    "\\[\n",
    "M_Y\\left(\\frac{t}{\\sqrt n}\\right)=1+\\frac{t^2}{2n}+o\\left(\\frac{1}{n}\\right).\n",
    "\\]\n",
    "\n",
    "Therefore\n",
    "\n",
    "\\[\n",
    "M_{Z_n}(t)=\\left(1+\\frac{t^2}{2n}+o\\left(\\frac{1}{n}\\right)\\right)^n \\to e^{t^2/2},\n",
    "\\]\n",
    "\n",
    "which is precisely the MGF of \\(\\mathcal N(0,1)\\). This proves the CLT under the stated conditions.\n",
    "\n",
    "### Why this motivates Gaussian increments\n",
    "\n",
    "If you add many tiny, weakly-dependent fluctuations, their aggregate (after proper normalization) becomes Gaussian.\n",
    "A *random walk* is literally a sum of i.i.d. steps, so its *macroscopic increments* become Gaussian — the core reason Brownian motion has Gaussian increments.\n"
   ],
   "id": "8a4445cb52628fa7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9) Summary: what to remember + common pitfalls\n\n### What to remember\n\n- **CLT**: sums of many small independent fluctuations become Gaussian after normalization.  \n  This is the fundamental reason **Gaussian increments** arise in diffusion limits.\n\n- **Random walk scaling**: to get a nontrivial continuous-time limit, you must scale steps as  \n  \\[\n  \\Delta x \\propto \\sqrt{\\Delta t}.\n  \\]\n  That is exactly Brownian scaling and leads to \\(\\mathrm{Var}(\\Delta W)=\\Delta t\\).\n\n- **What breaks from classical calculus**:\n  Brownian paths are continuous but **almost surely nowhere differentiable**, with **finite nonzero quadratic variation**:\n  \\[\n  \\sum ( \\Delta W )^2 \\to T.\n  \\]\n\n- **Itô’s lemma**:\n  because \\((dW_t)^2\\) behaves like \\(dt\\), the second-order Taylor term contributes at first order:\n  \\[\n  df = \\left(f_t + \\mu f_x + \\tfrac12 \\sigma^2 f_{xx}\\right)dt + \\sigma f_x dW_t.\n  \\]\n  That \\(\\tfrac12\\sigma^2 f_{xx}\\) is the **Itô correction**.\n\n- **Langevin geometry**:\n  \\[\n  dX_t = -\\nabla U(X_t)\\,dt + \\sqrt{2D}\\,dW_t\n  \\]\n  is **gradient descent** in the energy landscape plus isotropic exploration.\n  The stationary distribution is\n  \\[\n  p_\\infty(x)\\propto e^{-U(x)/D}.\n  \\]\n\n### Common pitfalls\n\n1. Treating \\(dW_t\\) like \\(dt\\): **wrong scale**. Remember \\(dW_t=O(\\sqrt{dt})\\).\n2. Dropping \\((dW_t)^2\\): **cannot** — it contributes at order \\(dt\\).\n3. Confusing diffusion coefficient: in \\(dX_t=\\sqrt{2D}\\,dW_t\\), variance over \\(\\Delta t\\) is \\(2D\\Delta t\\).\n4. Euler–Maruyama accuracy: decreasing \\(dt\\) improves Itô-identity checks; coarse \\(dt\\) creates discretization error.\n\n---\n\n### Where this connects to diffusion-based generative modeling (one line)\n\nIn score-based and diffusion generative models, reverse-time sampling is a controlled SDE/ODE whose noise structure is inherited from Brownian motion; understanding **Itô vs classical calculus** is what makes those derivations correct.\n",
   "id": "66382bf8ac9a1ea5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
